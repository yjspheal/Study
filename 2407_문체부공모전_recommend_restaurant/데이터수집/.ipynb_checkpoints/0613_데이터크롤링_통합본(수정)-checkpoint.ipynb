{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bdfa99",
   "metadata": {},
   "source": [
    "# 기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6fb9f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException\n",
    "from selenium.common.exceptions import ElementClickInterceptedException\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse, parse_qs, urlencode, urlunparse\n",
    "\n",
    "# 페이지 맨 아래까지 스크롤 다운하는 함수\n",
    "def scroll_down():\n",
    "    # 끝까지 스크롤 다운\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # 페이지 로드 대기\n",
    "        time.sleep(2)  # 페이지 로딩 대기 시간 조정 가능\n",
    "\n",
    "        # 새로운 높이 계산\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        # 더 이상 스크롤할 내용이 없으면 종료\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "# 텍스트 찾기 함수\n",
    "def find_all_text(x):\n",
    "    if x:\n",
    "        x_texts = [i.text.strip() for i in x]\n",
    "        x_text = ', '.join(x_texts)\n",
    "    else:\n",
    "        x_text = \"Not found\"\n",
    "    \n",
    "    return x_text\n",
    "\n",
    "# 메뉴 클릭 함수\n",
    "def click_menu(menu):\n",
    "    # 탭 클릭\n",
    "    try:\n",
    "    # CSS 선택자를 사용하여 'veBoZ' 클래스를 가진 <span> 요소 중 텍스트가 menu인 요소 찾기\n",
    "        menu_buttons = driver.find_elements(By.CSS_SELECTOR, 'a._tab-menu span.veBoZ')\n",
    "\n",
    "        # '메뉴' 텍스트를 가진 요소 클릭\n",
    "        for button in menu_buttons:\n",
    "            if menu in button.text:\n",
    "                button.click()\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"오류 발생: {e}\")\n",
    "        \n",
    "def scroll_up():\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(2)  # 스크롤 후 페이지 로딩 대기\n",
    "    \n",
    "def add_query_param_to_url(url, param_name, param_value):\n",
    "    \"\"\"현재 URL에 쿼리 파라미터를 추가하여 반환\"\"\"\n",
    "    url_parts = list(urlparse(url))\n",
    "    query = dict(parse_qs(url_parts[4]))\n",
    "    query[param_name] = param_value\n",
    "    url_parts[4] = urlencode(query, doseq=True)\n",
    "    return urlunparse(url_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dde475a",
   "metadata": {},
   "source": [
    "# 식당 홈 탭 크롤링\n",
    "- 식당 이름\n",
    "- 메뉴 카테고리\n",
    "- 주소\n",
    "- 전화번호\n",
    "- 웹사이트 주소\n",
    "- 영업 시간\n",
    "- 역에서부터의 거리\n",
    "- 서비스 목록\n",
    "- 총 리뷰 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43d7fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def home_page_data():\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # 영업시간 더보기 버튼 클릭\n",
    "    more_busshour_button = driver.find_elements(By.XPATH, '/html/body/div[3]/div/div/div/div[5]/div/div[2]/div[1]/div/div[2]/div/a/div/div')\n",
    "    if more_busshour_button:\n",
    "        more_busshour_button[0].click()\n",
    "        time.sleep(2)  # 클릭 후 로딩 대기\n",
    "    else:\n",
    "        pass  # 버튼이 없으면 넘어감\n",
    "    \n",
    "    scroll_down()\n",
    "\n",
    "    # 페이지 소스 가져오기\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # BeautifulSoup 객체 생성\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # 데이터 쌓기\n",
    "    # 가게 이름\n",
    "    try:\n",
    "        restaurant_name = soup.find('span', class_='GHAhO').text.strip()\n",
    "    except:\n",
    "        restaurant_name = float('nan')\n",
    "        \n",
    "    # 업종\n",
    "    try:\n",
    "        category = soup.find('span', class_='lnJFt').text.strip()\n",
    "    except:\n",
    "        category = float('nan')\n",
    "        \n",
    "    # 총 방문자 리뷰 수\n",
    "    try:\n",
    "        count_reviews = soup.find_all('em', class_='place_section_count')\n",
    "        second_count_review = count_reviews[1].text.strip()\n",
    "    except:\n",
    "        second_count_review = float('nan')\n",
    "        \n",
    "    # 주소\n",
    "    try:\n",
    "        address = soup.find('div', class_='O8qbU tQY7D').find('span', class_='LDgIH').text.strip()\n",
    "    except:\n",
    "        address = float('nan')\n",
    "       \n",
    "    # 역 기준 거리\n",
    "    try:\n",
    "        distance = soup.find('div', class_='nZapA').text.strip()\n",
    "    except:\n",
    "        distance = float('nan')\n",
    "\n",
    "    # 영업 시간\n",
    "    try:\n",
    "        business_hours = soup.find('a', class_='gKP9i RMgN0').get_text(separator=' ').strip()\n",
    "    except:\n",
    "        business_hours = float('nan')\n",
    "        \n",
    "\n",
    "    # 전화번호\n",
    "    try:\n",
    "        phone_number = soup.find('div', class_='O8qbU nbXkr').find('span', class_='xlx7Q').text.strip()\n",
    "    except:\n",
    "        phone_number = float('nan')\n",
    "        \n",
    "    # 홈페이지\n",
    "    try:\n",
    "        # 'div' 태그에서 class가 'O8qbU yIPfO'인 요소를 찾고, 그 안에 있는 'a' 태그를 모두 찾기\n",
    "        website_links = soup.find('div', class_='O8qbU yIPfO').find_all('a', class_='place_bluelink')\n",
    "        # website_links_hrefs = [link.get('href') for link in website_links]\n",
    "    except:\n",
    "        website_links = float('nan')\n",
    "        \n",
    "    # 부가 서비스 목록\n",
    "    try:\n",
    "        home_service = soup.find('div', class_='xPvPE').text.strip()\n",
    "    except:\n",
    "        home_service = float('nan')\n",
    "\n",
    "    return restaurant_name, category, second_count_review, address, distance, business_hours, phone_number, website_links, home_service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587bc44f",
   "metadata": {},
   "source": [
    "# 메뉴 탭 크롤링\n",
    "- 메뉴 텍스트 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30205946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def menu_page_data():\n",
    "    #메뉴 탭 클릭\n",
    "    click_menu('메뉴')\n",
    "    \n",
    "    time.sleep(5)\n",
    "    scroll_down()\n",
    "    \n",
    "    # 메뉴 더보기 버튼 클릭\n",
    "    while True:\n",
    "        try:\n",
    "            more_menu_button = driver.find_element(By.XPATH, '/html/body/div[3]/div/div/div/div[6]/div/div[1]/div[2]/div/a')\n",
    "            if more_menu_button.is_displayed() and more_menu_button.is_enabled():\n",
    "                more_menu_button.click()\n",
    "                time.sleep(2)  # 클릭 후 로딩 대기\n",
    "            else:\n",
    "                break\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "    \n",
    "    # 페이지 소스 가져오기\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # BeautifulSoup 객체 생성\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # 메뉴 텍스트 내용 긁어오기\n",
    "    try:\n",
    "        menu_texts = soup.find_all('div', class_='MXkFw')\n",
    "        if not menu_texts:  # 만약 찾은 결과가 비어있다면\n",
    "            raise ValueError(\"First selector not found\")\n",
    "        menu_contents = [menu_text.get_text(separator='//').strip() for menu_text in menu_texts]\n",
    "    except (AttributeError, ValueError):\n",
    "        try:\n",
    "            menu_texts = soup.find_all('div', class_='info_detail')\n",
    "            if not menu_texts:  # 만약 찾은 결과가 비어있다면\n",
    "                raise ValueError(\"Second selector not found\")\n",
    "            menu_contents = [menu_text.get_text(separator='//').strip() for menu_text in menu_texts]\n",
    "        except (AttributeError, ValueError):\n",
    "            menu_contents = float('nan')\n",
    "\n",
    "#     # 메뉴판 이미지\n",
    "#     menu_image_element = soup.find('div', class_='WKvXd').find('img')\n",
    "#     if menu_image_element is not None:\n",
    "#         menu_image = menu_image_element['src']\n",
    "#     else:\n",
    "#         menu_image = float('nan')\n",
    "\n",
    "    try:\n",
    "        menu_image_elements = soup.find_all('div', class_='WKvXd')\n",
    "        menu_images = []\n",
    "        for element in menu_image_elements:\n",
    "            img = element.find('img')\n",
    "            if img is not None:\n",
    "                menu_images.append(img['src'])\n",
    "            else:\n",
    "                menu_images.append(float('nan'))\n",
    "    except (AttributeError, TypeError, KeyError):\n",
    "        menu_images = [float('nan')]\n",
    "        \n",
    "    # 결과 반환\n",
    "    return menu_contents, menu_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162e831",
   "metadata": {},
   "source": [
    "# 정보 탭 크롤링\n",
    "- 소개글\n",
    "- 편의시설 및 서비스\n",
    "- 주차\n",
    "- 좌석, 공간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81341ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def info_page_data(ID):\n",
    "#     # 정보 탭 클릭\n",
    "#     click_menu('정보')\n",
    "\n",
    "    # 정보 링크로 이동\n",
    "    driver.get(f'https://m.place.naver.com/restaurant/{ID}/information')\n",
    "\n",
    "    time.sleep(5)\n",
    "    scroll_down()\n",
    "\n",
    "    # 페이지 소스 가져오기\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # BeautifulSoup 객체 생성\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # 데이터 쌓기\n",
    "    # 소개글\n",
    "    #     try:\n",
    "    #         introduce_button = driver.find_element(By.XPATH, '/html/body/div[3]/div/div/div/div[5]/div/div[1]/div/div/div[2]/a')\n",
    "    #         introduce_button.click()\n",
    "    #         introduce = soup.find('div', class_='T8RFa').text.strip()\n",
    "    #     except:\n",
    "    #         introduce = float('nan')\n",
    "    \n",
    "    # 편의 시설 및 서비스\n",
    "    try:\n",
    "        service = soup.find_all('div', class_='owG4q')\n",
    "        services = find_all_text(service)\n",
    "        \n",
    "    except:\n",
    "        services = float('nan')\n",
    "    \n",
    "    # 주차\n",
    "    try:\n",
    "        parking = soup.find('div', class_='TZ6eS').text.strip()\n",
    "    except:\n",
    "        parking = float('nan')\n",
    "    \n",
    "    # 좌석, 공간\n",
    "    try:\n",
    "        seat = soup.find_all('li', class_='Lw5L1')\n",
    "        seats = find_all_text(seat)\n",
    "        \n",
    "    except:\n",
    "        seats = float('nan')\n",
    "        \n",
    "    # 소개\n",
    "    try:\n",
    "        info = soup.find('div', class_=re.compile(r'^T8RFa')).text\n",
    "        \n",
    "    except:\n",
    "        info = float('nan')\n",
    "    \n",
    "    return services, parking, seats, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d445a3c",
   "metadata": {},
   "source": [
    "# 리뷰 탭 크롤링\n",
    "- 리뷰 텍스트 내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a66cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_page_data():\n",
    "    # 리뷰 탭 클릭\n",
    "    click_menu('리뷰')\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # scroll_up()\n",
    "    current_url = driver.current_url\n",
    "    updated_url = add_query_param_to_url(current_url, 'reviewSort', 'recent')\n",
    "    driver.get(updated_url)\n",
    "    time.sleep(5)  # 페이지 로딩 대기\n",
    "    \n",
    "    # 키워드 리뷰 데이터 쌓기\n",
    "    while True:\n",
    "        try:\n",
    "            # \"더보기\" 버튼의 XPath\n",
    "            more_review_keyword_xpath = '/html/body/div[3]/div/div/div/div[6]/div[2]/div[1]/div/div/div[2]/a[1]'\n",
    "\n",
    "            # \"더보기\" 버튼 요소 찾기\n",
    "            more_review_keyword = driver.find_element(By.XPATH, more_review_keyword_xpath)\n",
    "\n",
    "            # \"더보기\" 버튼 클릭\n",
    "            more_review_keyword.click()\n",
    "\n",
    "            # 클릭 후 페이지 로딩 대기\n",
    "            time.sleep(2)\n",
    "\n",
    "            # 'a' 태그의 'dP0sq' 클래스를 가진 요소 찾기\n",
    "            dP0sq_elements = driver.find_elements(By.CLASS_NAME, 'dP0sq')\n",
    "\n",
    "            if not dP0sq_elements:\n",
    "                # 'dP0sq' 클래스를 가진 요소가 없으면 루프 종료\n",
    "                break\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            # \"더보기\" 버튼을 찾을 수 없으면 루프 종료\n",
    "            break\n",
    "\n",
    "    scroll_down()\n",
    "\n",
    "    # 리뷰 키워드 정보 추출\n",
    "    # 페이지 소스 가져오기\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # BeautifulSoup 객체 생성\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # 모든 't3JSf' 태그와 'CUoLy' 태그를 찾음\n",
    "    review_keywords_elements = soup.find_all('span', class_='t3JSf')\n",
    "    review_keywords_count_elements = soup.find_all('span', class_='CUoLy')\n",
    "\n",
    "    # 두 리스트의 길이를 확인하여 최소 길이만큼 순회\n",
    "    min_length = min(len(review_keywords_elements), len(review_keywords_count_elements))\n",
    "\n",
    "    # 키워드 데이터 리스트 생성\n",
    "    review_keywords_data = []\n",
    "\n",
    "    # 순서대로 함께 추가\n",
    "    for i in range(min_length):\n",
    "        review_keywords = review_keywords_elements[i].text.strip()\n",
    "        review_keywords_count = review_keywords_count_elements[i].text.strip()\n",
    "        review_keywords_data.append({\n",
    "            'keyword': review_keywords,\n",
    "            'count': review_keywords_count\n",
    "        })\n",
    "    \n",
    "    scroll_down()\n",
    "    \n",
    "    # 끝까지 스크롤 다운 및 더보기 클릭 반복\n",
    "    i = 0\n",
    "    while i < 9:\n",
    "        # 끝까지 스크롤 다운\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        max_scroll_attempts = 1  # 최대 스크롤 시도 횟수 설정\n",
    "        scroll_attempts = 0\n",
    "\n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # 페이지 로딩 대기 시간 조정 가능\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                scroll_attempts += 1\n",
    "                if scroll_attempts >= max_scroll_attempts:\n",
    "                    break\n",
    "            else:\n",
    "                scroll_attempts = 0  # 높이가 변경되면 시도 횟수 초기화\n",
    "            last_height = new_height\n",
    "        \n",
    "        # 더보기 버튼 클릭\n",
    "        try:\n",
    "            more_review_button = driver.find_element(By.XPATH, '/html/body/div[3]/div/div/div/div[6]/div[2]/div[3]/div[2]/div/a')\n",
    "            # 더보기 버튼이 있는 곳으로 스크롤\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", more_review_button)\n",
    "            time.sleep(1)  # 스크롤 후 로딩 대기\n",
    "            if more_review_button.is_displayed() and more_review_button.is_enabled():\n",
    "                more_review_button.click()\n",
    "                i += 1\n",
    "                time.sleep(2)  # 클릭 후 로딩 대기\n",
    "                \n",
    "        except ElementClickInterceptedException:\n",
    "            # 스크롤을 조금씩 위로 올리면서 클릭을 시도\n",
    "            height = -100\n",
    "            while True:\n",
    "                driver.execute_script(f\"window.scrollBy(0, {height});\")\n",
    "                time.sleep(1)  # 스크롤 후 로딩 대기\n",
    "                try:\n",
    "                    more_review_button = driver.find_element(By.XPATH, '/html/body/div[3]/div/div/div/div[6]/div[2]/div[3]/div[2]/div/a')\n",
    "                    if more_review_button.is_displayed() and more_review_button.is_enabled():\n",
    "                        more_review_button.click()\n",
    "                        i += 1\n",
    "                        time.sleep(2)  # 클릭 후 로딩 대기\n",
    "                        break  # 클릭 성공 시 루프 탈출\n",
    "                except ElementClickInterceptedException:\n",
    "                    height -= 100  # height를 -100씩 더 줄임\n",
    "                except:\n",
    "                    break\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "        except ElementNotInteractableException:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    # 개별 리뷰 더보기 버튼\n",
    "    # 초기 XPath\n",
    "    base_xpath = '/html/body/div[3]/div/div/div/div[6]/div[2]/div[3]/div/ul/li[{}]/div/div[4]/a'\n",
    "\n",
    "    # 버튼을 순차적으로 클릭\n",
    "    button_index = 1\n",
    "    while True:\n",
    "        try:\n",
    "            # 현재 XPath\n",
    "            current_xpath = base_xpath.format(button_index)\n",
    "\n",
    "            # 해당 XPath에 해당하는 요소가 있는지 확인\n",
    "            button = driver.find_element(By.XPATH, current_xpath)\n",
    "\n",
    "            # 요소가 있다면 클릭하고 인덱스를 증가시킴\n",
    "            button.click()\n",
    "            time.sleep(2)  # 클릭 후 페이지 로딩 대기\n",
    "            button_index += 1\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    scroll_up()\n",
    "        \n",
    "    while True:\n",
    "        try:\n",
    "            # CSS 선택자를 사용하여 'sIv5s WPk67' 클래스를 가진 <a> 태그 찾기\n",
    "            review_buttons = driver.find_elements(By.CSS_SELECTOR, 'a.sIv5s.WPk67[role=\"button\"]')\n",
    "\n",
    "            if not review_buttons:\n",
    "                # 더 이상 클릭할 버튼이 없으면 종료\n",
    "                break\n",
    "\n",
    "            for button in review_buttons:\n",
    "                try:\n",
    "                    button.click()\n",
    "                    time.sleep(2)  # 클릭 후 페이지 로딩 대기\n",
    "                except ElementNotInteractableException:\n",
    "                    continue\n",
    "        except NoSuchElementException:\n",
    "            print(\"리뷰 더보기 버튼을 찾을 수 없습니다.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"오류 발생: {e}\")\n",
    "            break\n",
    "            \n",
    "    scroll_down()\n",
    "\n",
    "    # 페이지 로드 후 HTML 소스를 BeautifulSoup로 파싱\n",
    "    html_source = driver.page_source\n",
    "    soup = BeautifulSoup(html_source, 'html.parser')\n",
    "    \n",
    "    try:\n",
    "        review_texts = soup.find_all('span', class_='zPfVt')\n",
    "        keyword_review_texts = soup.find_all('div', class_='ERkm0')\n",
    "    except:\n",
    "        review_texts = float('nan')\n",
    "        keyword_review_texts = float('nan')\n",
    "\n",
    "    # 텍스트 추출\n",
    "    review_text_list = [span.get_text() for span in review_texts]\n",
    "    keyword_review_texts_list = [div.get_text(separator = ',') for div in keyword_review_texts]\n",
    "    \n",
    "    return review_keywords_data, review_text_list, keyword_review_texts_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd521b",
   "metadata": {},
   "source": [
    "### 크롤링 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a55eedc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665 중 1 완\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "16 columns passed, passed data had 17 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:939\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:986\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[1;34m(content, columns)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    987\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    989\u001b[0m     )\n\u001b[0;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[0;32m    991\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: 16 columns passed, passed data had 17 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# 결과를 데이터프레임으로 변환\u001b[39;00m\n\u001b[0;32m     36\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrestaurant_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond_count_review\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maddress\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbusiness_hours\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphone_number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwebsite_links\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_service\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmenu_contents\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservices\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparking\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseats\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_keywords_data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreview_text_list\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword_review_texts_list\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     40\u001b[0m ]\n\u001b[1;32m---> 41\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([combined_data], columns\u001b[38;5;241m=\u001b[39mcolumns)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 데이터프레임을 CSV 파일에 추가 (모드 'a'로 열고 헤더는 첫 번째에만 씀)\u001b[39;00m\n\u001b[0;32m     44\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:806\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    805\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m--> 806\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m nested_data_to_arrays(\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;66;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;00m\n\u001b[0;32m    808\u001b[0m         \u001b[38;5;66;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;00m\n\u001b[0;32m    809\u001b[0m         data,\n\u001b[0;32m    810\u001b[0m         columns,\n\u001b[0;32m    811\u001b[0m         index,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    812\u001b[0m         dtype,\n\u001b[0;32m    813\u001b[0m     )\n\u001b[0;32m    814\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[0;32m    815\u001b[0m         arrays,\n\u001b[0;32m    816\u001b[0m         columns,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    819\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[1;34m(data, columns, index, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    518\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[1;32m--> 520\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m to_arrays(data, columns, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    521\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:845\u001b[0m, in \u001b[0;36mto_arrays\u001b[1;34m(data, columns, dtype)\u001b[0m\n\u001b[0;32m    842\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m    843\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[1;32m--> 845\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m _finalize_columns_and_data(arr, columns, dtype)\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:942\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[1;34m(content, columns, dtype)\u001b[0m\n\u001b[0;32m    939\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    941\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[0;32m    945\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: 16 columns passed, passed data had 17 columns"
     ]
    }
   ],
   "source": [
    "# 데이터프레임 로드\n",
    "dong = pd.read_csv('창천동.csv')\n",
    "\n",
    "# Chrome WebDriver 초기화\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "\n",
    "# 결과 저장을 위한 빈 리스트 생성\n",
    "results = []\n",
    "\n",
    "totalnum = len(dong)\n",
    "file_path = '창천동_상세정보_ing.csv'\n",
    "\n",
    "# 기존 파일이 존재하면 삭제 (중복 방지)\n",
    "import os\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "\n",
    "# 각 ID에 대해 데이터를 수집하고 리스트에 저장\n",
    "for idx, row in dong.iterrows():\n",
    "    ID = row['ID']\n",
    "    url = f'https://m.place.naver.com/restaurant/{ID}/home?entry=pll'\n",
    "    driver.get(url)\n",
    "    \n",
    "    home_data = home_page_data()\n",
    "    menu_data = menu_page_data()\n",
    "    info_data = info_page_data(ID)\n",
    "    review_data = review_page_data()\n",
    "    \n",
    "    # 데이터를 하나의 튜플로 결합\n",
    "    combined_data = home_data + (menu_data,) + info_data + review_data\n",
    "    results.append(combined_data)\n",
    "\n",
    "    # 결과를 데이터프레임으로 변환\n",
    "    columns = [\n",
    "        'restaurant_name', 'category', 'second_count_review', 'address', 'distance', \n",
    "        'business_hours', 'phone_number', 'website_links', 'home_service', 'menu_contents', \n",
    "        'services', 'parking', 'seats', 'review_keywords_data', 'review_text_list', 'keyword_review_texts_list'\n",
    "    ]\n",
    "    results_df = pd.DataFrame([combined_data], columns=columns)\n",
    "\n",
    "    # 데이터프레임을 CSV 파일에 추가 (모드 'a'로 열고 헤더는 첫 번째에만 씀)\n",
    "    header = not os.path.isfile(file_path)\n",
    "    results_df.to_csv(file_path, mode='a', header=header, index=False)\n",
    "    \n",
    "    print(totalnum, '중', idx + 1, '완')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('창천동_상세정보.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "53acf15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_df.review_text_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a61fae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['제가 신촌에서 젤 좋아하는 카페예요 일단 자리가 편하고 요즘 트렌드 같은 예쁜 카페 느낌은 아니지만 아늑하고 특별한 메뉴가 있구!! ♥️ 핫초코 메뉴가 특별하게 당도 세 단계로 나눠져 있어요 이 카페를 좋아하는 게 제 개인적인 추억 때문도 있지만 아늑하고 편한 분위기를 좋아하는 분들은 분명 좋아하실 거 같아요',\n",
       " '신촌 유일무이 핫초코 맛집이에요\\n원래는 얼죽아여도 여기선 웬만하면 핫초코만 먹게 되는 거 같아요~ 카페 특유의 포근한 분위기와 메뉴, 사장님 모든 게 다 잘 어울리는 좋은 카페',\n",
       " '카페 추천받아서 왔는데 분위기랑 좌석이 넘 좋았어요!! 사장님도 짱짱 친절하셔서 몇 마디 얘기 안 나눴는데 기분이 좋아졌어요ㅎㅎ 노래 선곡도 너무 취저입니다🫶',\n",
       " '카페도 넓고 카공하기 딱 좋은 팟입니당 \\n시험기간에 항상 애용하고있어요😍😍',\n",
       " '레모네이드도 맛있었고 공부하기 좋았어요!',\n",
       " '',\n",
       " '신촌 최고의 카공 카페\\n주문메뉴: 단맛없는 다크초코 추천!',\n",
       " '참 차분하고 편안한 곳이네요. 소곤소곤 얘기를 나누어도 나쁘지 않고 모두들 아주 조용합니다. 음료는 맛도 최고고 더하기 정성이 가득하게 담겨있네요. \\n만남의 장소로도\\n혼자 보내는 장소로도\\n차 맛을 즐기는 곳으로도\\n다~ 좋아요 ~^^',\n",
       " '가끔 카공하러 가는데 사장님께서 완전 친절하시고 커피도 맛있어요! 아이스 초코도 자주 먹는데 강추합니다~',\n",
       " '좌석도 편하고 조용해서 좋아요:)',\n",
       " '차 종류가 많아요 사장님 차에 진심인듯...!\\n녹차 쉐이큰데 녹차향 가득하고 많이 안달아서 좋아요\\n얼그레이 좋아하시는 분 얼그레이 밀크티 꼭 드세요\\n입 안 가득 얼그레이 행복합니다',\n",
       " '라떼 jmt',\n",
       " '컵도 너무 귀엽고 사장님도 너무 친절하셔서 또 가고픈 카페입니다ㅠ 공부하기도 적절하고 도란도란 얘기나누기도 좋은 카페인것 같아요',\n",
       " '굿',\n",
       " '조용하고 좋아요 :)',\n",
       " '옛날 소개팅 장소로 선택했다면 백전백승이었을 것 같은 카페예요',\n",
       " '아지트같은 공간이에요.',\n",
       " '굿',\n",
       " '조용하고 좋은 매장입니다',\n",
       " '쉐이크가 맛있고 친절하세요!! ㅎㅎ',\n",
       " '매장이 좀 추워서 발이 많이 시려웠지만\\n조용하고 사람이 많지 않아서 좋았네요. 좌석도 넓고요. 초코음료 주문했는데 달기 조절 가능해서 너무 좋았어요.',\n",
       " '분위기 좋아요',\n",
       " '조아요',\n",
       " '좋아요',\n",
       " '굿',\n",
       " '친구랑 노가리까면서 카공하기조아요',\n",
       " '사장님도 친절하시고 음료도 정말 맛있어요! 분위기가 좋고 의자도 편해서 쉬기도 좋고 공부하기도 좋아요~~~',\n",
       " '여기 음료도 맛있고 분위기도 좋아요! 좌석도 편하고 학교에서 시간 때울 곳 없으면 종종 들리는 곳입니다~',\n",
       " '굿',\n",
       " '굿',\n",
       " '꼭 오세요',\n",
       " '포근하고 음료가 맛있어요 ',\n",
       " '카공 200%가능',\n",
       " '여기 쉐이크 맛있어요 쉐이크 먹으러 1시간 거리 찾아옴~',\n",
       " '너무너무 귀엽고 좋았어요 ~~~\\n음료들도 다 맛있고 사장님도 친절하세요!',\n",
       " '늘 오는 곳! 칭구도 데려왔는데 공부하기 좋다고 좋아해요 ><',\n",
       " '사장님 넘 친절하셔요',\n",
       " '맛있고 편안하고 친절하고 공부하기 좋은 카페',\n",
       " '편안해지고 싶을 때',\n",
       " 'ㅎㅎㅎ',\n",
       " '항상 감사해요~!',\n",
       " '커피가 진짜 맛있어요!',\n",
       " '중간 자리에 콘센트가 없었는데도 사장님께서 세팅해주셔서 편하게 일할 수 있었어요. 너무 감사드립니다!☺',\n",
       " '조용히 대화하기 좋아요~\\n밀크쉐이크랑 아이스초코도 맛있게 먹었습니다!!\\n사장님이 친절하셔서 자주 오게됩니다^^',\n",
       " '맛있어요!!',\n",
       " '녹차랑 초코랑 진짜 맛있었어요❣ 일하거나 공부하기 좋은 카페인 듯합니다. 조곤조곤하게 이야기하실 분들도 좋은 거 같아요!',\n",
       " '너무조아여',\n",
       " '카공하기 완벽한 장소! 뷰도 너무 좋고 노래소리도 작아요 저만 알고 있고 싶어용ㅎㅎ',\n",
       " '맛있고 분위기도조용해요',\n",
       " '자리도 편하고 음료도 맛있었어요 ',\n",
       " '녹차도 상당히 진하고 쉽게 녹지 말라고 큰 얼음을 주신다는 것도 좋았어요! 또 친환경 생분해성 빨대 쓰시는 것도 너무 좋았습니다👍👍',\n",
       " '바닐라모카 너무 맛있었어요 주변에 프랜차이즈가 너무많아서 모르고 지나쳤던 곳인데 적당한 화이트초코 같은 베이스에 바닐라빈 들어간 시럽까지 달달하고 창문으로 보이는 벚꽃도 너무 예뻤어요 요즘 우울했는데 덕분에 기분전환이 잘 되었어요 감사드려요 또 올게요',\n",
       " '조용히 대화하기 좋습니다.',\n",
       " '앉는 자리도 편하고 커피도 직접 가져다주시면서 친절하고 좋았어요!',\n",
       " '사장님이 친절하네요\\n장소도 아늑하고 잘 쉬다갑니다',\n",
       " '녹차 쉐이크 꼭 먹어볼 것',\n",
       " '좋았습니다~ 공부할때면 매번 찾는곳!',\n",
       " '변하지 않는 맛',\n",
       " '편안한 분위기와 맛있는 커피의 조화:)',\n",
       " '사장님이 엄청 친절하시고 커피랑 밀크쉐이크 진짜 맛집입니다... 그리고 카페 플레이리스트도 너무 좋아요ㅠㅠ 공부하러 가기도 좋고, 그냥 도란도란 대화하거나 책읽거나 멍때리기도 좋습니다!!! 신촌 최애 카페에용❤️',\n",
       " '맛있어요!!',\n",
       " '항상 맛있는 커피 감사합니다!',\n",
       " '맛있어요 공부하기도 좋아요!',\n",
       " '시나몬 추가했는데 맛이 잘 안났던것같아여\\n그래도 맛있었어요~카페도 편안하고 좋네용 또 갈게요~~',\n",
       " '초코 안달고 맛있어요',\n",
       " '녹차도 맛있고 사장님도 너무 친절해요ㅎㅎ',\n",
       " '사장님이 친절하세요. 거의 갈때마다 밀크셰이크로 먹어요. ',\n",
       " '맛도 좋고 분위기도 좋았어요👍🏻',\n",
       " '담백하고 고소해요!',\n",
       " '맛있어요 ~~',\n",
       " '쾌적하고 시끄럽지 않고 좋아요!! 음료도 양 많아요👍',\n",
       " '밀크쉐이크 맛집!!',\n",
       " '맛있었어요',\n",
       " '넘나 맛잇듭미다!!!!!!!',\n",
       " '나무좋아요',\n",
       " '오랜만에 갔는데 여전히 조용하고 분위기 좋았어요 ㅎㅎ',\n",
       " '평소엔 그냥 샷1개만 넣어서 연하게 먹었는데 사이즈업 연하게가 옵션으로 나와서 좋아요 :) ',\n",
       " '이게 제일맛잇어요',\n",
       " '맛있어요~~!!',\n",
       " '신촌에서 제일 좋아하는 카페, 사장님도 친절하세요 🙌🏼',\n",
       " '카페도 아늑하고 좋아요',\n",
       " '초코맜있어요!',\n",
       " '늘 맛있고 조용하고 친절하셔요!',\n",
       " '더운 날씨에 편하게 쉬고 갑니다!\\n사장님 항상 친절하세요~',\n",
       " '너무 좋아하구 자주 찾는 카페에요 !!',\n",
       " '넘 맛있고 카페 분위기도 너무 좋아요~~',\n",
       " '불친절해요',\n",
       " '넘 좋아하는 카페에요!',\n",
       " '좋아용 소파도 편하고',\n",
       " '주인분이 친절하세요']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.review_text_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e448b255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
