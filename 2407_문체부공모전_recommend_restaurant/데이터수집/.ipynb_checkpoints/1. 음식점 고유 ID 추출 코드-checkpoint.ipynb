{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb22bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 import\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3dfb54",
   "metadata": {},
   "source": [
    "## 크롤링하고자 하는 음식 카테고리 지정\n",
    "- 네이버지도 검색에 '00역 음식점', '00역 맛집' 등 검색 시 등장하는 음식 카테고리 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77883559",
   "metadata": {},
   "source": [
    "### 음식 카테고리 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f6978c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 검색 결과가 표시되는 iframe으로 전환\n",
    "def search_iframe():\n",
    "    driver.switch_to.default_content() # 차상위 (기본) 프레임으로 전환\n",
    "    driver.switch_to.frame(\"searchIframe\") # 검색 결과 프레임으로 전환\n",
    "\n",
    "def entry_iframe():\n",
    "    driver.switch_to.default_content() # 차상위 (기본) 프레임으로 전환\n",
    "    WebDriverWait(driver, 2).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"entryIframe\"]'))) # WebDriverWait : entryIframe이 나타날 때까지 대기\n",
    "\n",
    "    # for 문으로 총 5번까지 시도, 0.5초간 대기 후 프레임 전환을 시도 -> frame 전환에 성공하면 break 문으로 빠져나온다\n",
    "    for i in range(5):\n",
    "        time.sleep(1.5)\n",
    "        try:\n",
    "            driver.switch_to.frame(driver.find_element(By.XPATH, '//*[@id=\"entryIframe\"]'))\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# 화면에 표시된 업체명을 확인하는 함수\n",
    "def chk_names():\n",
    "    search_iframe() # search 프레임으로 전환\n",
    "    elem = driver.find_elements(By.XPATH, '//*[@id=\"_pcmap_list_scroll_container\"]/ul/li/div[1]/a[1]/div/div/span[1]') # 해당 프레임에서 Xpath를 사용하여 업체명이 표시된 요소를 찾음 원래는 //*[@id=\"_pcmap_list_scroll_container\"]/ul/li/div[1]/div/a[1]/div/div/span[1]\n",
    "    name_list = [e.text for e in elem] # find_elements 함수로 업체명이 표시된 요소를 모두 찾고, 그 중 text 값을 가져와서 리스트에 저장\n",
    "\n",
    "    return elem, name_list\n",
    "\n",
    "\n",
    "def chk_details():\n",
    "    entry_iframe() # 세부 정보의 iframe으로 전환\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#     print(soup)\n",
    "    # 데이터 쌓기 - 업종, 주소, url 순\n",
    "    try:\n",
    "        category = soup.find('span', {'class': 'lnJFt'})\n",
    "    except:\n",
    "        category = float('nan')\n",
    "\n",
    "\n",
    "    try:\n",
    "        # 특정 meta 태그를 찾기\n",
    "        meta_tag = soup.find('meta', {'id': 'og:url'})\n",
    "\n",
    "        # URL에서 숫자 부분만 추출\n",
    "        if meta_tag:\n",
    "            url = meta_tag['content']\n",
    "            ID = url.split('/')[4]\n",
    "    except:\n",
    "        ID = float('nan')\n",
    "\n",
    "    search_iframe() # 다시 검색 결과의 iframe으로 전환\n",
    "\n",
    "    return category, url, ID\n",
    "\n",
    "def crawling_main():\n",
    "    global naver_res # 전역변수 naver_res 사용 (크롤링 데이터를 저장하는 데이터프레임)\n",
    "    category_list = [] # 업종 리스트\n",
    "    url_list = [] # url 리스트\n",
    "    ID_list = []\n",
    "\n",
    "    for e in elem:\n",
    "        try:\n",
    "            e.click()\n",
    "            category, url, ID = chk_details() # 업체의 세부 정보 가져오기\n",
    "        except:\n",
    "            category = ''\n",
    "            url=''\n",
    "            ID =''\n",
    "        \n",
    "\n",
    "        # 가져온 정보를 리스트에 추가\n",
    "        category_list.append(category)\n",
    "        url_list.append(url)\n",
    "        ID_list.append(ID)\n",
    "\n",
    "    naver_temp = pd.DataFrame([name_list, category_list, url_list, ID_list], index = naver_res.columns).T\n",
    "    naver_res = pd.concat([naver_res, naver_temp])\n",
    "    naver_res.to_excel('./naver_crawling_result.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "# webdriver 실행\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "keyword = '신촌역 맛집'\n",
    "url = f'https://map.naver.com/p/search/{keyword}'\n",
    "driver.get(url)\n",
    "# 마우스 및 키보드 동작 시뮬레이터\n",
    "action = ActionChains(driver)\n",
    "# 불러올 정보를 담을 빈 데이터프레임 생성\n",
    "naver_res = pd.DataFrame(columns=['업체명','업종','URL','ID'])\n",
    "\n",
    "last_name = ''\n",
    "\n",
    "page_num = 1\n",
    "\n",
    "while True:\n",
    "    time.sleep(1.5)\n",
    "    search_iframe()\n",
    "    elem, name_list = chk_names()\n",
    "    if last_name == name_list[-1]:\n",
    "        break\n",
    "    while True:\n",
    "        # 자동 스크롤 구현부\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", elem[-1])\n",
    "        time.sleep(1.5)\n",
    "        elem, name_list = chk_names()\n",
    "        if last_name == name_list[-1]:\n",
    "            break\n",
    "        else:\n",
    "            last_name = name_list[-1]\n",
    "    crawling_main()\n",
    "    # 다음 페이지로 이동\n",
    "    try:\n",
    "        next_button = driver.find_elements(By.CLASS_NAME, 'eUTV2')[-1]\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659669e7",
   "metadata": {},
   "source": [
    "### 검색 시 등장한 음식 카테고리 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761bc269",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set(naver_res['업종'].apply(lambda x: x.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ce46c",
   "metadata": {},
   "source": [
    "### 크롤링 대상 음식 카테고리 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94cb744",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_categories = ['게요리', '곱창,막창,양', '과일,주스전문점', '국밥', '국수', '김밥', '낙지요리', '냉면', \n",
    "    '다이어트,샐러드', '닭갈비', '닭발', '도시락,컵밥', '돈가스', '돼지고기구이', '두부요리', \n",
    "    '딤섬,중식만두', '떡볶이', '만두', '매운탕,해물탕', '맥주,호프', '바(BAR)', '베이글', '베이커리', \n",
    "    '베트남음식', '북카페', '분식', '브런치카페', '샌드위치', '생선회', '샤브샤브', '소고기구이', \n",
    "    '스테이크,립', '스파게티,파스타전문', '아시아음식', '양갈비', '양꼬치', '양식', '오뎅,꼬치', \n",
    "    '와인', '와플', '요리주점', '우동,소바', '육류,고기요리', '이자카야', '이탈리아음식', '인도음식', \n",
    "    '일식당', '일식튀김,꼬치', '장어,먹장어요리', '조개요리', '족발,보쌈', '종합분식', '주꾸미요리', \n",
    "    '중식당', '찌개,전골', '초밥,롤', '치킨,닭강정', '카페', '카페,디저트', '칼국수,만두', '케이크전문', \n",
    "    '태국음식', '테이크아웃커피', '토스트', '포장마차', '퓨전음식', '프랑스음식', '피자', '한식', \n",
    "    '한식뷔페', '한정식-일반', '해물,생선요리', '해장국', '햄버거', '감자탕', '곰탕,설렁탕', '기사식당', \n",
    "    '덮밥', '라면', '문래돼지불백', '백반,가정식', '백숙,삼계탕', '복어요리', '빙수', '순대,순댓국', \n",
    "    '아이스크림', '오므라이스', '죽', '찜닭', '추어탕', '카레', '패밀리레스토랑', '향토음식', \n",
    "    '홍차전문점', '후렌치후라이', '호떡', '쌈밥', '아귀찜,해물찜', '닭볶음탕', '대게요리', '마라탕', \n",
    "    '멕시코,남미음식', '브런치', '스페인음식', '정육식당', '전통,민속주점', '일본식라면']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75fb545",
   "metadata": {},
   "source": [
    "## 크롤링하고자 하는 동 지정 (ex_ 아현동)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bfadb04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thdth\\AppData\\Local\\Temp\\ipykernel_14908\\933161894.py:2: DtypeWarning: Columns (41,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  restaurant = pd.read_csv(\"./서울시마포구일반음식점인허가정보.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각자 동에 맞는 거로!\n",
    "restaurant = pd.read_csv(\"./서울시마포구일반음식점인허가정보.csv\")\n",
    "restaurant = restaurant[restaurant['영업상태명'] == '영업/정상']\n",
    "\n",
    "# NaN 값을 빈 문자열로 대체, 안 하면 에러남\n",
    "restaurant['지번주소'] = restaurant['지번주소'].fillna('')\n",
    "\n",
    "\n",
    "# '지번주소' 열이 '서울특별시 마포구 --동'으로 시작하는 행만 선택\n",
    "target_dong = '서울특별시마포구아현동'\n",
    "#dong = restaurant[restaurant['지번주소'].str.startswith(target_dong)]\n",
    "dong = restaurant[restaurant['지번주소'].str.replace(' ', '').str.startswith(target_dong)]\n",
    "\n",
    "# dong에서 도로명주소를 기준으로 검색할 것이기 때문에, 주소 기준 중복인 행을 다 없앰\n",
    "dong = dong.drop_duplicates(subset='도로명주소', keep=False)\n",
    "dong.reset_index(drop=True, inplace=True)\n",
    "len(dong) # 해당 동의 음식점 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4966260",
   "metadata": {},
   "source": [
    "## 크롤링하고자 하는 동 내 음식점 ID 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776bc023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# webdriver 실행\n",
    "#driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "driver = webdriver.Chrome()\n",
    "# 검색 결과가 표시되는 iframe으로 전환\n",
    "def search_iframe():\n",
    "    driver.switch_to.default_content()  # 차상위 (기본) 프레임으로 전환\n",
    "    try:\n",
    "        time.sleep(1.5)  # 페이지 로딩 대기\n",
    "        driver.switch_to.frame(\"searchIframe\")  # 검색 결과 프레임으로 전환\n",
    "    except NoSuchElementException:\n",
    "        print(\"searchIframe을 찾을 수 없습니다.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# 개별 업체의 세부 정보가 표시되는 iframe으로 전환\n",
    "def entry_iframe():\n",
    "    driver.switch_to.default_content()  # 차상위 (기본) 프레임으로 전환\n",
    "    try:\n",
    "        time.sleep(1.5)  # 페이지 로딩 대기\n",
    "        driver.switch_to.frame(\"entryIframe\")  # 세부 정보 프레임으로 전환\n",
    "    except NoSuchElementException:\n",
    "        print(\"entryIframe을 찾을 수 없습니다.\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# type1의 화면에 표시된 업체명을 확인하는 함수\n",
    "def chk_names_type1():\n",
    "    search_iframe()  # search 프레임으로 전환\n",
    "    elems = driver.find_elements(By.CLASS_NAME, 'place_bluelink.C6RjW')\n",
    "    name_list = []\n",
    "    category_list = []\n",
    "    elements = []\n",
    "    \n",
    "    for e in elems:\n",
    "        try:\n",
    "            name = e.find_element(By.CLASS_NAME, 'YwYLL').text\n",
    "            category = e.find_element(By.CLASS_NAME, 'YzBgS').text\n",
    "            \n",
    "            # 업종이 target_categories에 포함되지 않으면 패스\n",
    "            if category not in target_categories:\n",
    "                continue\n",
    "                \n",
    "            name_list.append(name)\n",
    "            category_list.append(category)\n",
    "            elements.append(e)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return elements, name_list, category_list\n",
    "\n",
    "# type2의 화면에 표시된 업체명을 확인하는 함수\n",
    "def chk_names_type2(processed_names):\n",
    "    elems = driver.find_elements(By.CLASS_NAME, 'search_box')\n",
    "    name_list = []\n",
    "    category_list = []\n",
    "    elements = []\n",
    "    \n",
    "    for e in elems:\n",
    "        try:\n",
    "            name = e.find_element(By.CLASS_NAME, 'search_title').text\n",
    "            category = e.find_element(By.CLASS_NAME, 'category').text\n",
    "            \n",
    "            # 이미 처리된 이름은 패스\n",
    "            if name in processed_names:\n",
    "                continue\n",
    "            \n",
    "            # 업종이 target_categories에 포함되지 않으면 패스\n",
    "            if category not in target_categories:\n",
    "                continue\n",
    "                \n",
    "            name_list.append(name)\n",
    "            category_list.append(category)\n",
    "            elements.append(e)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return elements, name_list, category_list\n",
    "\n",
    "# 세부 페이지에서 URL과 ID를 추출하는 함수 (type1)\n",
    "def chk_details():\n",
    "    if not entry_iframe():  # 세부 정보의 iframe으로 전환\n",
    "        return float('nan'), float('nan')\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    try:\n",
    "        # 특정 meta 태그를 찾기\n",
    "        meta_tag = soup.find('meta', {'id': 'og:url'})\n",
    "\n",
    "        # URL에서 숫자 부분만 추출\n",
    "        if meta_tag:\n",
    "            url = meta_tag['content']\n",
    "            ID = url.split('/')[4]\n",
    "    except:\n",
    "        url = float('nan')\n",
    "        ID = float('nan')\n",
    "\n",
    "    search_iframe()  # 다시 검색 결과의 iframe으로 전환\n",
    "    return url, ID\n",
    "\n",
    "# 세부 페이지에서 URL과 ID를 추출하는 함수 (type2)\n",
    "def chk_details_type2():\n",
    "    if not entry_iframe():  # 세부 정보의 iframe으로 전환\n",
    "        return float('nan'), float('nan')\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    try:\n",
    "        # 특정 meta 태그를 찾기\n",
    "        meta_tag = soup.find('meta', {'id': 'og:url'})\n",
    "\n",
    "        # URL에서 숫자 부분만 추출\n",
    "        if meta_tag:\n",
    "            url = meta_tag['content']\n",
    "            ID = url.split('/')[4]\n",
    "    except:\n",
    "        url = float('nan')\n",
    "        ID = float('nan')\n",
    "\n",
    "    return url, ID\n",
    "\n",
    "# 크롤링 메인 함수\n",
    "def crawling_main(type):\n",
    "    global naver_res  # 전역변수 naver_res 사용 (크롤링 데이터를 저장하는 데이터프레임)\n",
    "    global target_dong\n",
    "    \n",
    "    processed_names = set()  # 이미 처리된 업체명을 추적하기 위한 집합\n",
    "    \n",
    "    if type == \"type1\":\n",
    "        elements, name_list, category_list = chk_names_type1()\n",
    "        details_func = chk_details\n",
    "        \n",
    "        id_list = []\n",
    "        url_list = []\n",
    "\n",
    "        for e in elements:\n",
    "            try:\n",
    "                e.click()  # 업체명 클릭\n",
    "                time.sleep(1.5)  # 페이지 로딩 대기\n",
    "                url, ID = details_func()  # 업체의 세부 정보 가져오기\n",
    "                url_list.append(url)\n",
    "                id_list.append(ID)\n",
    "\n",
    "            except:\n",
    "                url_list.append(float('nan'))\n",
    "                id_list.append(float('nan'))\n",
    "                continue\n",
    "        \n",
    "        # 새로운 데이터프레임 생성\n",
    "        naver_temp = pd.DataFrame({\n",
    "            '지번주소': [current_jibun] * len(name_list),\n",
    "            '도로명주소': [current_keyword] * len(name_list),\n",
    "            '업체명': name_list,\n",
    "            '업종': category_list,\n",
    "            'URL': url_list,\n",
    "            'ID': id_list\n",
    "        })\n",
    "        \n",
    "        # 기존 데이터프레임과 결합\n",
    "        naver_res = pd.concat([naver_res, naver_temp])\n",
    "        naver_res.reset_index(drop=True, inplace=True)\n",
    "        naver_res.to_csv(f'./{target_dong.split()[-1]}.csv')  # 결과를 csv 파일로 저장\n",
    "        \n",
    "    if type == \"type2\":\n",
    "        all_name_list = []\n",
    "        all_category_list = []\n",
    "        all_id_list = []\n",
    "        all_url_list = []\n",
    "        \n",
    "        while True:\n",
    "            elements, name_list, category_list = chk_names_type2(processed_names)\n",
    "            details_func = chk_details_type2\n",
    "\n",
    "            if len(elements) == 0:\n",
    "                break\n",
    "\n",
    "            for e, name, category in zip(elements, name_list, category_list):\n",
    "                try:\n",
    "                    e.click()  # 업체명 클릭\n",
    "                    time.sleep(1.5)  # 페이지 로딩 대기\n",
    "                    url, ID = details_func()  # 업체의 세부 정보 가져오기\n",
    "                    all_url_list.append(url)\n",
    "                    all_id_list.append(ID)\n",
    "                    processed_names.add(name)  # 처리된 이름을 추가\n",
    "                    all_name_list.append(name)\n",
    "                    all_category_list.append(category)\n",
    "                    driver.back()  # 뒤로가기 버튼 클릭\n",
    "                    time.sleep(1.5)  # 뒤로가기 후 페이지 로딩 대기\n",
    "                except:\n",
    "                    continue\n",
    "        \n",
    "        # 새로운 데이터프레임 생성\n",
    "        naver_temp = pd.DataFrame({\n",
    "            '지번주소': [current_jibun] * len(all_name_list),\n",
    "            '도로명주소': [current_keyword] * len(all_name_list),\n",
    "            '업체명': all_name_list,\n",
    "            '업종': all_category_list,\n",
    "            'URL': all_url_list,\n",
    "            'ID': all_id_list\n",
    "        })\n",
    "\n",
    "        # 기존 데이터프레임과 결합\n",
    "        naver_res = pd.concat([naver_res, naver_temp])\n",
    "        naver_res.to_csv(f'./{target_dong.split()[-1]}.csv')  # 결과를 csv 파일로 저장\n",
    "\n",
    "# 불러올 정보를 담을 빈 데이터프레임 생성\n",
    "naver_res = pd.DataFrame(columns=['지번주소', '도로명주소', '업체명', '업종', 'URL', 'ID'])\n",
    "\n",
    "for index, row in dong.iterrows():\n",
    "    current_jibun = row['지번주소']  # '지번주소'를 저장\n",
    "    current_keyword = row['도로명주소']  # '도로명주소'를 검색 키워드로 사용\n",
    "    \n",
    "    print(f'전체 {len(dong)} 중 {index+1}, 주소:{current_jibun}')\n",
    "    \n",
    "    url = f'https://map.naver.com/p/search/{current_keyword}'\n",
    "    driver.get(url)\n",
    "\n",
    "    # \"sc-1wsjitl dunggE overlap\" 클래스가 있는지 확인\n",
    "    try:\n",
    "        time.sleep(1.5)  # 페이지 로딩 대기\n",
    "        overlap_element = driver.find_element(By.CLASS_NAME, 'sc-1wsjitl.dunggE.overlap')\n",
    "        try:\n",
    "            more_button = overlap_element.find_element(By.CLASS_NAME, 'link_more')\n",
    "            more_button.click()\n",
    "            time.sleep(1.5)  # \"더보기\" 버튼 클릭 후 페이지 로딩 대기\n",
    "        except NoSuchElementException:\n",
    "            pass  # \"더보기\" 버튼이 없는 경우 그냥 넘어감\n",
    "        crawling_main(\"type2\")\n",
    "    except NoSuchElementException:\n",
    "        try:\n",
    "            no_result = driver.find_element(By.CLASS_NAME, 'correction_result_text')\n",
    "            continue\n",
    "        except:\n",
    "            try:\n",
    "                crawling_main(\"type1\")\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "\n",
    "    action = ActionChains(driver)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90c6af",
   "metadata": {},
   "source": [
    "## 중복행 제거 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복행 제거\n",
    "naver_res_drop = naver_res.drop_duplicates(subset='ID', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일로 저장\n",
    "naver_res_drop.to_csv('0611_naver_res_drop.csv', index=False, encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.988px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
